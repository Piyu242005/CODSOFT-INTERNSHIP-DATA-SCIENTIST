{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Credit Card Fraud Detection\n",
                "\n",
                "**Author:** Piyush Ramteke  \n",
                "**Program:** CodSoft Data Science Internship  \n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Problem Statement\n",
                "\n",
                "Credit card fraud causes billions of dollars in losses every year. As digital payments grow, so does fraudulent activity. The challenge is to **automatically detect fraudulent transactions** among hundreds of thousands of legitimate ones.\n",
                "\n",
                "This is a **binary classification** problem:\n",
                "- **Class 0** → Genuine transaction\n",
                "- **Class 1** → Fraudulent transaction\n",
                "\n",
                "The key difficulty is **class imbalance** — frauds make up less than 0.2% of all transactions. A naive model that predicts everything as \"genuine\" would get 99.8% accuracy but catch zero frauds.\n",
                "\n",
                "**Goal:** Build a model that maximizes **Recall** (catching as many frauds as possible) while maintaining acceptable **Precision** (minimizing false alarms)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dataset Overview\n",
                "\n",
                "The dataset contains credit card transactions made by European cardholders in **September 2013** over a two-day period.\n",
                "\n",
                "| Feature | Description |\n",
                "|---------|-------------|\n",
                "| `Time` | Seconds elapsed since the first transaction |\n",
                "| `V1` – `V28` | PCA-transformed features (anonymized for privacy) |\n",
                "| `Amount` | Transaction amount |\n",
                "| `Class` | **Target** — 0 = Genuine, 1 = Fraud |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Import Libraries ──────────────────────────────────────────\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    classification_report, confusion_matrix,\n",
                "    accuracy_score, precision_score, recall_score, f1_score,\n",
                "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
                ")\n",
                "from imblearn.over_sampling import SMOTE\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "plt.rcParams['font.size'] = 12\n",
                "\n",
                "print('All libraries loaded.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Load Dataset ─────────────────────────────────────────────\n",
                "\n",
                "df = pd.read_csv('creditcard.csv')\n",
                "\n",
                "print(f'Shape: {df.shape[0]:,} rows × {df.shape[1]} columns')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Dataset Info ─────────────────────────────────────────────\n",
                "\n",
                "df.info()\n",
                "print()\n",
                "df[['Time', 'Amount', 'Class']].describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Check for Missing Values ─────────────────────────────────\n",
                "\n",
                "missing = df.isnull().sum().sum()\n",
                "print(f'Total missing values: {missing}')\n",
                "print('→ The dataset has no missing values.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Observations:**\n",
                "- 284,807 transactions with 31 columns\n",
                "- `V1`–`V28` are already PCA-scaled; `Time` and `Amount` are not\n",
                "- No missing values — the dataset is clean\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploratory Data Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 3.1 Class Distribution ───────────────────────────────────\n",
                "\n",
                "counts = df['Class'].value_counts()\n",
                "pct    = df['Class'].value_counts(normalize=True) * 100\n",
                "\n",
                "print('Class Distribution:')\n",
                "print(f'  Genuine (0): {counts[0]:>7,}   ({pct[0]:.3f}%)')\n",
                "print(f'  Fraud   (1): {counts[1]:>7,}   ({pct[1]:.3f}%)')\n",
                "print(f'\\n  Ratio: 1 fraud per {counts[0]//counts[1]} genuine transactions')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 3.2 Visualize Class Imbalance ────────────────────────────\n",
                "\n",
                "colors = ['#27ae60', '#c0392b']\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
                "\n",
                "# Bar chart\n",
                "bars = axes[0].bar(['Genuine (0)', 'Fraud (1)'], counts.values,\n",
                "                   color=colors, edgecolor='black')\n",
                "axes[0].set_title('Class Distribution', fontsize=15, fontweight='bold')\n",
                "axes[0].set_ylabel('Count')\n",
                "for b, c in zip(bars, counts.values):\n",
                "    axes[0].text(b.get_x() + b.get_width()/2, b.get_height()+2000,\n",
                "                f'{c:,}', ha='center', fontweight='bold', fontsize=12)\n",
                "\n",
                "# Pie chart\n",
                "axes[1].pie(counts, labels=['Genuine','Fraud'], autopct='%1.3f%%',\n",
                "            colors=colors, explode=(0.02, 0.12), shadow=True,\n",
                "            textprops={'fontsize':12})\n",
                "axes[1].set_title('Class Proportion', fontsize=15, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Interpretation:** The dataset is **extremely imbalanced** — only 0.173% of transactions are fraudulent. Any model trained on this data directly will be biased toward predicting \"genuine\" and will fail to detect fraud. We will address this in **Section 5**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 3.3 Transaction Amount Distribution ──────────────────────\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Histogram (zoom to $0–$500)\n",
                "axes[0].hist(df[df['Class']==0]['Amount'], bins=60, alpha=0.7,\n",
                "             color=colors[0], label='Genuine', edgecolor='black')\n",
                "axes[0].hist(df[df['Class']==1]['Amount'], bins=60, alpha=0.7,\n",
                "             color=colors[1], label='Fraud', edgecolor='black')\n",
                "axes[0].set_xlim(0, 500)\n",
                "axes[0].set_title('Transaction Amount (≤ $500)', fontsize=15, fontweight='bold')\n",
                "axes[0].set_xlabel('Amount ($)')\n",
                "axes[0].set_ylabel('Frequency')\n",
                "axes[0].legend()\n",
                "\n",
                "# Boxplot\n",
                "sns.boxplot(x='Class', y='Amount', data=df, palette=colors, ax=axes[1])\n",
                "axes[1].set_ylim(0, 400)\n",
                "axes[1].set_xticklabels(['Genuine', 'Fraud'])\n",
                "axes[1].set_title('Amount by Class', fontsize=15, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f'Genuine → Mean: ${df[df[\"Class\"]==0][\"Amount\"].mean():.2f}, '\n",
                "      f'Median: ${df[df[\"Class\"]==0][\"Amount\"].median():.2f}')\n",
                "print(f'Fraud   → Mean: ${df[df[\"Class\"]==1][\"Amount\"].mean():.2f}, '\n",
                "      f'Median: ${df[df[\"Class\"]==1][\"Amount\"].median():.2f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Interpretation:** Fraudulent transactions tend to have **lower median amounts** compared to genuine ones. Most fraud occurs at smaller transaction values, making it harder to detect by amount alone."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 3.4 Top Features Correlated with Fraud ───────────────────\n",
                "\n",
                "corr = df.corr()['Class'].drop('Class').abs().sort_values(ascending=False)\n",
                "top10 = corr.head(10)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "top10.plot(kind='barh', ax=ax, color='#2980b9', edgecolor='black')\n",
                "ax.set_title('Top 10 Features Correlated with Fraud', fontsize=15, fontweight='bold')\n",
                "ax.set_xlabel('Absolute Correlation')\n",
                "ax.invert_yaxis()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Interpretation:** Features like `V17`, `V14`, `V12`, and `V10` have the strongest correlations with the fraud label. Since these are PCA components, they represent hidden patterns in the original transaction data that distinguish fraud from genuine activity.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 4.1 Normalize Amount and Time ────────────────────────────\n",
                "# V1–V28 are already PCA-scaled.\n",
                "# Amount and Time are on very different scales, so we\n",
                "# standardize them (mean=0, std=1) to match.\n",
                "\n",
                "scaler = StandardScaler()\n",
                "\n",
                "df['Scaled_Amount'] = scaler.fit_transform(df[['Amount']])\n",
                "df['Scaled_Time']   = scaler.fit_transform(df[['Time']])\n",
                "\n",
                "# Drop original unscaled columns\n",
                "df_processed = df.drop(['Amount', 'Time'], axis=1)\n",
                "\n",
                "print('Scaled_Amount — '\n",
                "      f'mean: {df_processed[\"Scaled_Amount\"].mean():.4f}, '\n",
                "      f'std: {df_processed[\"Scaled_Amount\"].std():.4f}')\n",
                "print('Scaled_Time   — '\n",
                "      f'mean: {df_processed[\"Scaled_Time\"].mean():.4f}, '\n",
                "      f'std: {df_processed[\"Scaled_Time\"].std():.4f}')\n",
                "print('\\nAmount and Time normalized successfully.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 4.2 Separate Features and Target ────────────────────────\n",
                "\n",
                "X = df_processed.drop('Class', axis=1)\n",
                "y = df_processed['Class']\n",
                "\n",
                "print(f'Features shape: {X.shape}')\n",
                "print(f'Target shape:   {y.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 4.3 Stratified Train/Test Split ─────────────────────────\n",
                "# stratify=y ensures both sets maintain the same fraud ratio.\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f'Training set: {X_train.shape[0]:,} samples  '\n",
                "      f'(Genuine: {(y_train==0).sum():,}, Fraud: {(y_train==1).sum()})')\n",
                "print(f'Testing set:  {X_test.shape[0]:,} samples  '\n",
                "      f'(Genuine: {(y_test==0).sum():,}, Fraud: {(y_test==1).sum()})')\n",
                "print(f'\\nFraud ratio preserved — '\n",
                "      f'Train: {(y_train==1).mean()*100:.3f}%, '\n",
                "      f'Test: {(y_test==1).mean()*100:.3f}%')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Summary of Preprocessing:**\n",
                "- No missing values found\n",
                "- `Amount` and `Time` standardized using **StandardScaler**\n",
                "- Stratified 80/20 split preserves original fraud ratio in both sets\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Handling Class Imbalance\n",
                "\n",
                "We use **SMOTE (Synthetic Minority Oversampling Technique)** to balance the training data.\n",
                "\n",
                "**How SMOTE works:**\n",
                "1. For each fraud sample, it finds its k-nearest fraud neighbors\n",
                "2. It creates new synthetic fraud samples by interpolating between them\n",
                "3. This produces realistic, diverse fraud examples — better than simple duplication\n",
                "\n",
                "> **Important:** SMOTE is applied **only on the training set**. The test set remains untouched to simulate real-world conditions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 5.1 Apply SMOTE ─────────────────────────────────────────\n",
                "\n",
                "smote = SMOTE(random_state=42)\n",
                "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
                "\n",
                "print('Before SMOTE:')\n",
                "print(f'  Genuine: {(y_train==0).sum():,}  |  Fraud: {(y_train==1).sum()}')\n",
                "\n",
                "print('\\nAfter SMOTE:')\n",
                "print(f'  Genuine: {(y_train_sm==0).sum():,}  |  Fraud: {(y_train_sm==1).sum():,}')\n",
                "print(f'  Total:   {len(y_train_sm):,}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 5.2 Visualize Before vs After SMOTE ─────────────────────\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
                "\n",
                "pd.Series(y_train).value_counts().plot(\n",
                "    kind='bar', ax=axes[0], color=colors, edgecolor='black')\n",
                "axes[0].set_title('Before SMOTE', fontsize=15, fontweight='bold')\n",
                "axes[0].set_xticklabels(['Genuine', 'Fraud'], rotation=0)\n",
                "axes[0].set_ylabel('Count')\n",
                "\n",
                "pd.Series(y_train_sm).value_counts().plot(\n",
                "    kind='bar', ax=axes[1], color=colors, edgecolor='black')\n",
                "axes[1].set_title('After SMOTE', fontsize=15, fontweight='bold')\n",
                "axes[1].set_xticklabels(['Genuine', 'Fraud'], rotation=0)\n",
                "axes[1].set_ylabel('Count')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Result:** After SMOTE, both classes are perfectly balanced. The model can now learn fraud patterns without being overwhelmed by the majority class.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Training\n",
                "\n",
                "We train two classifiers:\n",
                "\n",
                "| Model | Why |\n",
                "|-------|-----|\n",
                "| **Logistic Regression** | Simple, fast, interpretable — good baseline |\n",
                "| **Random Forest** | Ensemble of decision trees — captures complex patterns |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 6.1 Train Logistic Regression ───────────────────────────\n",
                "\n",
                "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
                "lr.fit(X_train_sm, y_train_sm)\n",
                "\n",
                "lr_pred = lr.predict(X_test)\n",
                "lr_prob = lr.predict_proba(X_test)[:, 1]\n",
                "\n",
                "print('Logistic Regression — trained.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 6.2 Train Random Forest ─────────────────────────────────\n",
                "\n",
                "rf = RandomForestClassifier(\n",
                "    n_estimators=100,   # number of trees\n",
                "    max_depth=15,       # limit depth to prevent overfitting\n",
                "    random_state=42,\n",
                "    n_jobs=-1           # use all CPU cores\n",
                ")\n",
                "rf.fit(X_train_sm, y_train_sm)\n",
                "\n",
                "rf_pred = rf.predict(X_test)\n",
                "rf_prob = rf.predict_proba(X_test)[:, 1]\n",
                "\n",
                "print('Random Forest — trained.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Both models are trained on **SMOTE-balanced data** and evaluated on the **original imbalanced test set** to reflect real-world performance.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Model Evaluation\n",
                "\n",
                "### Why Recall Matters Most in Fraud Detection\n",
                "\n",
                "| Metric | Meaning | In fraud context |\n",
                "|--------|---------|------------------|\n",
                "| **Precision** | % of predicted frauds that are real | High precision → fewer false alarms |\n",
                "| **Recall** | % of real frauds that we caught | **High recall → fewer missed frauds** |\n",
                "| **F1-Score** | Harmonic mean of precision and recall | Best single metric for imbalanced data |\n",
                "\n",
                "**Key insight:** A missed fraud (False Negative) means **real financial loss**. A false alarm (False Positive) is just a phone call to the customer. Therefore, **Recall is the priority metric.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 7.1 Confusion Matrices ──────────────────────────────────\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "for ax, name, pred in [\n",
                "    (axes[0], 'Logistic Regression', lr_pred),\n",
                "    (axes[1], 'Random Forest', rf_pred)\n",
                "]:\n",
                "    cm = confusion_matrix(y_test, pred)\n",
                "    sns.heatmap(cm, annot=True, fmt=',d', cmap='Blues', ax=ax,\n",
                "                xticklabels=['Genuine','Fraud'],\n",
                "                yticklabels=['Genuine','Fraud'],\n",
                "                annot_kws={'fontsize': 14})\n",
                "    ax.set_title(name, fontsize=14, fontweight='bold')\n",
                "    ax.set_ylabel('Actual')\n",
                "    ax.set_xlabel('Predicted')\n",
                "\n",
                "plt.suptitle('Confusion Matrices', fontsize=17, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print('Reading guide:')\n",
                "print('  Top-left  = True Negative  (Genuine → Genuine)  ✓')\n",
                "print('  Top-right = False Positive (Genuine → Fraud)    ← false alarm')\n",
                "print('  Bot-left  = False Negative (Fraud → Genuine)    ← MISSED fraud!')\n",
                "print('  Bot-right = True Positive  (Fraud → Fraud)      ✓ caught!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 7.2 Classification Reports ──────────────────────────────\n",
                "\n",
                "print('=' * 55)\n",
                "print('Logistic Regression — Classification Report')\n",
                "print('=' * 55)\n",
                "print(classification_report(y_test, lr_pred,\n",
                "                            target_names=['Genuine','Fraud']))\n",
                "\n",
                "print('=' * 55)\n",
                "print('Random Forest — Classification Report')\n",
                "print('=' * 55)\n",
                "print(classification_report(y_test, rf_pred,\n",
                "                            target_names=['Genuine','Fraud']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 7.3 ROC Curves ──────────────────────────────────────────\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(9, 7))\n",
                "\n",
                "for name, prob, clr in [\n",
                "    ('Logistic Regression', lr_prob, '#2980b9'),\n",
                "    ('Random Forest', rf_prob, '#c0392b')\n",
                "]:\n",
                "    fpr, tpr, _ = roc_curve(y_test, prob)\n",
                "    auc = roc_auc_score(y_test, prob)\n",
                "    ax.plot(fpr, tpr, color=clr, lw=2,\n",
                "            label=f'{name} (AUC = {auc:.4f})')\n",
                "\n",
                "ax.plot([0,1],[0,1], 'k--', lw=1, alpha=0.5, label='Random (AUC = 0.5)')\n",
                "ax.set_xlabel('False Positive Rate', fontsize=13)\n",
                "ax.set_ylabel('True Positive Rate (Recall)', fontsize=13)\n",
                "ax.set_title('ROC Curve', fontsize=16, fontweight='bold')\n",
                "ax.legend(fontsize=11)\n",
                "ax.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 7.4 Precision-Recall Curves ─────────────────────────────\n",
                "# More informative than ROC for imbalanced datasets.\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(9, 7))\n",
                "\n",
                "for name, prob, clr in [\n",
                "    ('Logistic Regression', lr_prob, '#2980b9'),\n",
                "    ('Random Forest', rf_prob, '#c0392b')\n",
                "]:\n",
                "    p, r, _ = precision_recall_curve(y_test, prob)\n",
                "    ap = average_precision_score(y_test, prob)\n",
                "    ax.plot(r, p, color=clr, lw=2,\n",
                "            label=f'{name} (AP = {ap:.4f})')\n",
                "\n",
                "ax.set_xlabel('Recall', fontsize=13)\n",
                "ax.set_ylabel('Precision', fontsize=13)\n",
                "ax.set_title('Precision-Recall Curve', fontsize=16, fontweight='bold')\n",
                "ax.legend(fontsize=11)\n",
                "ax.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Interpretation:**\n",
                "- The **ROC curve** shows overall discriminative ability — closer to the top-left is better.\n",
                "- The **Precision-Recall curve** is more relevant for imbalanced problems because it focuses on the fraud class specifically.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 8.1 Build Comparison Table ──────────────────────────────\n",
                "\n",
                "def get_metrics(name, y_true, y_pred, y_prob):\n",
                "    return {\n",
                "        'Model': name,\n",
                "        'Accuracy':  accuracy_score(y_true, y_pred),\n",
                "        'Precision': precision_score(y_true, y_pred),\n",
                "        'Recall':    recall_score(y_true, y_pred),\n",
                "        'F1-Score':  f1_score(y_true, y_pred),\n",
                "        'ROC-AUC':   roc_auc_score(y_true, y_prob)\n",
                "    }\n",
                "\n",
                "results = pd.DataFrame([\n",
                "    get_metrics('Logistic Regression', y_test, lr_pred, lr_prob),\n",
                "    get_metrics('Random Forest',       y_test, rf_pred, rf_prob)\n",
                "]).set_index('Model').round(4)\n",
                "\n",
                "print('Final Model Comparison:')\n",
                "print('=' * 65)\n",
                "results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 8.2 Bar Chart Comparison ────────────────────────────────\n",
                "\n",
                "metrics = ['Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
                "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
                "\n",
                "bar_colors = ['#2980b9', '#c0392b']\n",
                "\n",
                "for i, m in enumerate(metrics):\n",
                "    ax = axes[i]\n",
                "    vals = results[m].values\n",
                "    bars = ax.bar(results.index, vals, color=bar_colors, edgecolor='black')\n",
                "    ax.set_title(m, fontsize=14, fontweight='bold')\n",
                "    ax.set_ylim(0, 1.15)\n",
                "    ax.tick_params(axis='x', rotation=15)\n",
                "    for b, v in zip(bars, vals):\n",
                "        ax.text(b.get_x()+b.get_width()/2, b.get_height()+0.02,\n",
                "                f'{v:.4f}', ha='center', fontweight='bold', fontsize=11)\n",
                "\n",
                "plt.suptitle('Model Performance Comparison',\n",
                "             fontsize=18, fontweight='bold', y=1.04)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── 8.3 Best Model Selection ────────────────────────────────\n",
                "\n",
                "best = results['F1-Score'].idxmax()\n",
                "\n",
                "print('Best Model (by F1-Score):', best)\n",
                "print(f'  Precision: {results.loc[best, \"Precision\"]:.4f}')\n",
                "print(f'  Recall:    {results.loc[best, \"Recall\"]:.4f}')\n",
                "print(f'  F1-Score:  {results.loc[best, \"F1-Score\"]:.4f}')\n",
                "print(f'  ROC-AUC:   {results.loc[best, \"ROC-AUC\"]:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 9. Conclusion\n",
                "\n",
                "### Key Findings\n",
                "\n",
                "1. **The dataset is heavily imbalanced** (0.17% fraud). Accuracy is misleading — a model predicting all-genuine achieves 99.8% accuracy but catches zero fraud.\n",
                "\n",
                "2. **SMOTE effectively balanced the training data** by generating synthetic fraud samples, allowing models to learn fraud patterns without losing genuine data.\n",
                "\n",
                "3. **Both Logistic Regression and Random Forest** performed well after SMOTE balancing. Random Forest generally achieves higher precision, while Logistic Regression may achieve higher recall depending on the threshold.\n",
                "\n",
                "4. **Recall is the most important metric** for fraud detection:\n",
                "   - A **missed fraud** (False Negative) = real financial loss to the customer and bank.\n",
                "   - A **false alarm** (False Positive) = minor inconvenience, resolved with a verification call.\n",
                "   - Therefore, we prioritize catching every fraud even at the cost of some false alarms.\n",
                "\n",
                "5. **F1-Score** provides the best single metric for this problem, balancing precision and recall.\n",
                "\n",
                "### Recommendations\n",
                "\n",
                "- Use **threshold tuning** to adjust the precision-recall tradeoff for business needs\n",
                "- Consider **Gradient Boosting** (XGBoost, LightGBM) for potential improvements\n",
                "- Implement **cost-sensitive learning** to weight fraud misclassification more heavily\n",
                "- **Retrain periodically** — fraud patterns evolve over time\n",
                "\n",
                "---\n",
                "\n",
                "*Project by **Piyush Ramteke** — CodSoft Data Science Internship*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}